{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "873ce102",
   "metadata": {},
   "source": [
    "# Biological Inspiration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0497ec8e",
   "metadata": {},
   "source": [
    "**Inspired By Neurons:**  \n",
    "Perceptron mimics the basic function of biological neurons in the brain\n",
    "\n",
    "**Biological Neuron:**\n",
    "- **Dendrites:** Act as input receivers, collecting signals from other neurons.\n",
    "- **Activation:** If the combined input signals exceed a threshold, the neuron fires a signal.\n",
    "- **Axon:** Transmits the signal to other neurons.\n",
    "- **Synapse:** The junction between an axon and a dendrite, where synaptic strength acts like a weight, modulating the signal.\n",
    "\n",
    "*Note: The perceptron simplifies this biological process into a computational model.*\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/biological_neuron.png\" alt=\"Biological Motivation Behind Perceptron\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b9dd08",
   "metadata": {},
   "source": [
    "# McCulloch-Pitts Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74883852",
   "metadata": {},
   "source": [
    "An early model of artificial neurons with **binary inputs** and **threshold-based activation**.\n",
    "\n",
    "**Components:**\n",
    "- Input vector: $X = [x_1, \\cdots, x_d]$, where each $x_i \\in \\{0,1\\}$\n",
    "- Weights vector: $W = [w_1, \\cdots, w_d]$\n",
    "- Threshold: $\\theta$\n",
    "\n",
    "**Pre-Activation:**  \n",
    "Compute the weighted sum:\n",
    "$$z = \\sum_{i=1}^d w_i x_i$$\n",
    "\n",
    "**Activation function (Threshold-Based):**\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "1 & \\text{if } z \\ge \\theta \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\quad\n",
    "\\text{where } z = \\sum_i w_i x_i\n",
    "$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/neuron_first.png\" alt=\"simple ai neuron example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc6ef1",
   "metadata": {},
   "source": [
    "**Bias Alternative:**  \n",
    "Instead of a threshold $\\theta$, introduce a bias term $b = -\\theta$, shifting the activation function to:\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "1 & \\text{if } \\sum_{i=1}^d w_i x_i + b \\geq 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/neuron_second.png\" alt=\"better ai neuron example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc9557",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b852945",
   "metadata": {},
   "source": [
    "A generalization of the McCulloch-Pitts neuron that:\n",
    "- Accepts **real-valued inputs**\n",
    "- Supports **customizable activation functions**\n",
    "\n",
    "**Activation Function Recommendations:**\n",
    "- Non-zero derivative in most regions (important for gradient-based training)\n",
    "- Differentiable, to allow gradient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194faf9",
   "metadata": {},
   "source": [
    "## common Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a19dac",
   "metadata": {},
   "source": [
    "### **Step function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e58ee",
   "metadata": {},
   "source": [
    "**Formula:**\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "1 & \\text{if } x \\ge 0 \\\\ \n",
    "0 & \\text{if } x \\lt 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Issues:**\n",
    "- Not differentiable at $x=0$\n",
    "- Derivative is $0$ elsewhere $\\implies$ **Not suitable** for gradient-based learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19552336",
   "metadata": {},
   "source": [
    "### **Sigmoid (Logistic) Function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a37e51",
   "metadata": {},
   "source": [
    "Also known as **soft** perceptron\n",
    "\n",
    "**Activation Function:**\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad \\text{Where: } z = \\sum_i w_ix_i + b$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/sigmoid.png\" alt=\"sigmoid function\">\n",
    "</div>\n",
    "\n",
    "**Key Properties:**\n",
    "- It is a good candidate for activation function\n",
    "- It gives us a number between 0 and 1 smoothly\n",
    "- Smooth & differentiable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b6ab3",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197cc04",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cff00b",
   "metadata": {},
   "source": [
    "**Definition:**  \n",
    "A network of perceptron organized in layers, also called a **feed-forward neural network**.\n",
    "\n",
    "$$\\text{Input Layer} \\rightarrow \\underbrace{\\underbrace{\\text{Hidden Layer $1$} \\rightarrow \\cdots \\rightarrow \\text{Hidden Layer $n$}}_{\\text{Hidden Layers}} \\rightarrow \\text{Output Layer}}_{\\text{Network Layers (Computational Layers)}}$$\n",
    "input Layer\n",
    "\n",
    "- **Computational Layers** = Hidden Layers + Output Layer\n",
    "\n",
    "- **Network Depth** = Number of computational layers\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/mlp_example.png\" alt=\"MLP example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61f1bf",
   "metadata": {},
   "source": [
    "## MLP Computation Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dcec4b",
   "metadata": {},
   "source": [
    "Suppose we have a Multi-Layer Neural Network:\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/mlp_weighted.png\" alt=\"MLP example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb850778",
   "metadata": {},
   "source": [
    "**Layer 1 Computation:**\n",
    "$$z_{j}^{[1]} = \\phi_1 \\left(\\sum_{i=0}^d w_{ji}^{[1]} x_i \\right)$$\n",
    "equivalently in the matrix form:\n",
    "$$z = \\phi_1 (W^{[1]} x)$$\n",
    "\n",
    "Where:\n",
    "- $x$: Input vector\n",
    "\n",
    "- $W^{[1]}$: Weights of first layer\n",
    "\n",
    "- $\\phi_1$: Activation function for layer 1\n",
    "\n",
    "- $\\sum_{i=0}^dw_ji^{[1]} x_i$ is pre-activation function\n",
    "\n",
    "- $z^{[1]}$: Output of first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d203ec51",
   "metadata": {},
   "source": [
    "**Layer 2 Computation:**\n",
    "$$z_{j} = \\phi_2 (\\sum_{i=0}^d w_ji^{[2]} \\phi_1 (W^{[1]} x))$$\n",
    "equivalently in the matrix form:\n",
    "$$z = \\phi_2 (W^{[2]} \\phi_1 (W^{[1]} x))$$\n",
    "\n",
    "Each layer applies a linear transformation followed by a non-linear activation.\n",
    "\n",
    "*Usually, all hidden layers use the same activation function.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f719245f",
   "metadata": {},
   "source": [
    "**Importance of Non-linear Activation:**\n",
    "\n",
    "If no non-linear activation is used:\n",
    "\n",
    "$$z^{[1]} = W^{[1]} x$$\n",
    "$$z^{[2]} = W^{[2]} W^{[1]} x = W' x$$\n",
    "\n",
    "This is equivalent to a single linear layer. **No extra modeling power is added.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31b9bac",
   "metadata": {},
   "source": [
    "# Deep vs. Shallow Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d116b4af",
   "metadata": {},
   "source": [
    "**Depth of a Network:**  \n",
    "Defined as the shortest path (number of computational layers) from any input node to any output node in the DAG of the network.\n",
    "\n",
    "The network is deep if it has more than one hidden layers (i.e., more than 2 computational layers)\n",
    "\n",
    "- **Width**:\n",
    "  - More neurons per layer increase the model's capacity and complexity.\n",
    "\n",
    "- **Depth**:\n",
    "  - More layers allow greater abstraction and modeling of complex patterns\n",
    "  - Deep networks (more than one hidden layer) are considered **deep learning**\n",
    "\n",
    "- **Balance**:\n",
    "    - **Too narrow/shallow**: risk of underfitting\n",
    "    - **Too wide/deep**: risk of overfitting\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/width_depth_balance.png\" alt=\"width-depth balance\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907fb512",
   "metadata": {},
   "source": [
    "# MLP as Universal Approximator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b63e44",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15670e0",
   "metadata": {},
   "source": [
    "**Theorem:**  \n",
    "A feed-forward neural network with a single hidden layer and linear output can approximate any continuous function on a compact subset of $R^D$ to arbitrary accuracy.\n",
    "\n",
    "**Conditions:**\n",
    "- Activation function must be non-linear and satisfy mild assumptions (e.g., continuous, bounded, non-constant, such as sigmoid or ReLU).\n",
    "- Requires a sufficiently large (but finite) number of hidden units\n",
    "\n",
    "**Mathematical Representation:** For a network with $M$ hidden units:\n",
    "$$\n",
    "F_k(x) = \\sum_{j=1}^M w_{kj}^{[2]} \\phi \\left(\\sum_{i=1}^d w{ji}^{[1]} x_i \\right)\n",
    "$$\n",
    "Where:\n",
    "- $x \\in \\mathbb{R}^D$: Input vector.\n",
    "\n",
    "- $w_{ji}^{[1]}$: Weights from input to hidden layer.\n",
    "\n",
    "- $b_j^{[1]}$: Bias for hidden units.\n",
    "\n",
    "- $\\phi$: Non-linear activation function (e.g., sigmoid, ReLU).\n",
    "\n",
    "- $w_{kj}^{[2]}, b_k^{[2]}$: Weights and biases for the output layer.\n",
    "\n",
    "\n",
    "**Note:** This is of more theoretical interest than practical use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768839fd",
   "metadata": {},
   "source": [
    "## MLPs as Universal Boolean Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b9471c",
   "metadata": {},
   "source": [
    "### **Core Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f414f5f8",
   "metadata": {},
   "source": [
    "**Key Idea:**  \n",
    "MLPs can represent any Boolean function by constructing networks for basic logic gates (**AND**, **OR**, **NOT**) and **combining them**.\n",
    "\n",
    "**Basic Gates:**\n",
    "- **AND Gate**:\n",
    "    - Single-layer network with threshold $\\theta$ set between 1 and 2.\n",
    "\n",
    "- **OR Gate**:\n",
    "    - Single-layer network with threshold $\\theta$ between 0 and 1.\n",
    "\n",
    "- **NOT Gate**:  \n",
    "    - Single neuron with weight $w = -1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bc791f",
   "metadata": {},
   "source": [
    "### **Generalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6ba1af",
   "metadata": {},
   "source": [
    "Any Boolean function can be expressed in CNF or disjunctive normal form (DNF), making MLPs universal for Boolean functions.\n",
    "\n",
    "Given $N$ Boolean variables:\n",
    "- **Number of neurons** depends on the representation **complexity**\n",
    "- More compact representations reduce required neurons\n",
    "- One such compact tool: **Karnaugh Map**\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/karnaugh_map.png\" alt=\"Karnaugh map example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4534d5",
   "metadata": {},
   "source": [
    "### **Largest Irreducible Boolean Function: Parity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef3744c",
   "metadata": {},
   "source": [
    "**Parity Function:**  \n",
    "Computes whether the number of 1s in $D$ inputs is odd or even.\n",
    "\n",
    "For a single hidden layer:\n",
    "- Requires $2^{D-1}$ hidden units.\n",
    "- Total weights and biases: $(D + 2) \\cdot 2^{D-1} + 1$.\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/single_layer_parity.png\" alt=\"single layer parity\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be314c8",
   "metadata": {},
   "source": [
    "**Deep Network for Parity**\n",
    "\n",
    "**Recursive Construction Formula:**\n",
    "$$f = (((X_1 \\oplus X_2) \\oplus X_3) \\cdots \\oplus X_D)$$\n",
    "- Each XOR requires 3 neurons (2 hidden, 1 output).\n",
    "\n",
    "*Please Check XOR part if you have any confusion about this*\n",
    "\n",
    "**Resource Requirements:**\n",
    "- **Total Nodes:** $3 (D - 1)$\n",
    "- **Weights & biases:** $9 (D - 1)$\n",
    "\n",
    "Example for $D = 4$:\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/deep_network_parity.png\" alt=\"deep network parity\">\n",
    "</div>\n",
    "\n",
    "**Layer Breakdown:**\n",
    "- Layer 1:  \n",
    "    $Result1 = X_1 \\oplus X_2\\rightarrow 3 nodes$\n",
    "\n",
    "- Layer 2:  \n",
    "    $Result2 = Result1 \\oplus X_3 \\rightarrow 3 nodes$\n",
    "\n",
    "- Layer 3:  \n",
    "    $Result3 = Result2 \\oplus X_4 \\rightarrow 3 nodes$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e0d5d5",
   "metadata": {},
   "source": [
    "**Depth vs. Width Tradeoff:**\n",
    "\n",
    "Optimal architecture balances width and depth based on the function’s complexity.\n",
    "\n",
    "**Better Architecture Example:**\n",
    "$$f = ((X_1 \\oplus X_2) \\oplus (X_3\\oplus X_4)) \\oplus ((X_4 \\oplus X_5) \\oplus (X_6\\oplus X_7))$$\n",
    "\n",
    "Requires $2 \\log N$ layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b95520",
   "metadata": {},
   "source": [
    "### **Summary: Wide vs. Deep Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba527a",
   "metadata": {},
   "source": [
    "- A **one-layer MLP** can represent any Boolean function (**universal** Boolean function)\n",
    "    - But may require an **exponential** number of neurons\n",
    "- Deep networks can represent the same function with **fewer parameters**\n",
    "    - Depth gives **efficiency**\n",
    "- Complexity depends on:\n",
    "    - Number of variables\n",
    "    - Minimum DNF/CNF size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775e0e04",
   "metadata": {},
   "source": [
    "### **Limit: Shannon’s Theorem**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749bc96",
   "metadata": {},
   "source": [
    "- For $D > 2$, some Boolean functions require at least $\\frac{2^D}{D}$ gates in a Boolean circuit.\n",
    "\n",
    "- **Implication:** For large $D$, most $D$-input Boolean functions need exponentially large circuits, regardless of depth.\n",
    "\n",
    "- **Note:** If all Boolean functions could be computed with polynomial-sized circuits, it would imply $P = NP$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4fb3a",
   "metadata": {},
   "source": [
    "**Threshold Circuits (TCs):**\n",
    "\n",
    "- MLPs use threshold gates, which are more powerful than standard Boolean gates.\n",
    "- Example: A single threshold gate can compute the majority function (“at least $K$ inputs are 1”), which requires an exponential-sized Boolean circuit.\n",
    "- For fixed depth: Boolean Circuits $\\subset$ Threshold Circuits.\n",
    "- Threshold Circuits can also be viewed as Arithmetic Circuits\n",
    "    - Model computation over reals (not just Boolean values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104999a",
   "metadata": {},
   "source": [
    "## MLPs as Universal Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5926a47f",
   "metadata": {},
   "source": [
    "**Problem Overview**\n",
    "\n",
    "- MLP as a function over real inputs\n",
    "- MLP as a function that finds a complex “decision boundary” over a space of reals\n",
    "- Output is binary or multi-class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914802af",
   "metadata": {},
   "source": [
    "| Structure                     | Type of Decision Regions      | Interpretation                     |\n",
    "|-------------------------------|-------------------------------|------------------------------------|\n",
    "| Single Layer (no hidden layer) | Half space                   | Region found by a hyper-plane      |\n",
    "| Two Layer (one hidden layer)   | Polyhedral (open or closed)   | Intersection of half spaces        |\n",
    "| Three Layer (two hidden layers)| Arbitrary regions            | Union of polyhedrals               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete this part later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be37569b",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/convex_classifier.png\" alt=\"convex classifier\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39141706",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/non_convex_classifier.png\" alt=\"non-convex classifier\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ca0f64",
   "metadata": {},
   "source": [
    "**Polygon-Based Region Approximation**\n",
    "\n",
    "- Let $M$ = number of sides\n",
    "- As $M$ increases, approximation to a circular region improves\n",
    "  - Reduces the area outside the polygon that have $\\frac{M}{2} \\lt \\text{Sum} \\lt M$\n",
    "\n",
    "$$\\sum_i z_i = M \\left( 1 - \\frac{1}{\\pi} \\arccos\\left(\\min\\left(1, \\frac{\\text{radius}}{|x - \\text{cent}|}\\right)\\right) \\right)$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/cylinder_region.png\" alt=\"cylinder\">\n",
    "</div>\n",
    "\n",
    "- Inside: Sum = $M$\n",
    "- Outside: Sum = $\\frac{M}{2}$\n",
    "- Apply bias: $b = -\\frac{M}{2}$\n",
    "\n",
    "Result: Network can isolate a circle-like region with high precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b98f40",
   "metadata": {},
   "source": [
    "The circle net\n",
    "- Very large number of neurons\n",
    "- Sum is $M$ inside the circle, $\\frac{M}{2}$ outside almost everywhere\n",
    "    - After adding bias $-\\frac{M}{2}$\n",
    "        - Sum is $\\frac{M}{2}$ inside the circle, $0$ outside almost everywhere\n",
    "- Circle can be at any location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b0af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete thjis part later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c098c5e",
   "metadata": {},
   "source": [
    "## MLPs as Universal Approximator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df17c43",
   "metadata": {},
   "source": [
    "**Goal:** MLP as a continuous-valued regression\n",
    "\n",
    "A simple 3-unit MLP with a **summing** output unit can generate a **square pulse** over an input\n",
    "- Output is 1 only if the input lies between $T_1$ and $T_2$\n",
    "- $T_1$ and $T_2$ can be arbitrarily specified\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/square_pulse.png\" alt=\"square pulse\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7da110d",
   "metadata": {},
   "source": [
    "An MLP with many units can model an arbitrary function over an input\n",
    "- Simply make the individual pulses narrower\n",
    "\n",
    "**Result:** A one-layer MLP can model an arbitrary function of a single input\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/continuous_value_regression.png\" alt=\"continuous value regression example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adcb1ac",
   "metadata": {},
   "source": [
    "# MLP with ReLU Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05188f1",
   "metadata": {},
   "source": [
    "**ReLU Activation:**\n",
    "$$\\phi(z) = \\max(0, z)$$\n",
    "\n",
    "**Effect:**  \n",
    "Creates piecewise linear functions.\n",
    "\n",
    "**Number of Pieces:**\n",
    "- Proportional to the number of neurons in the layer.\n",
    "- Increasing neurons increases the number of linear segments, enabling finer approximations.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/relu_pieces.png\" alt=\"relation between ReLU function and pieces\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9978b44f",
   "metadata": {},
   "source": [
    "### **Composing Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba244837",
   "metadata": {},
   "source": [
    "What is the effect of increasing layers?\n",
    "\n",
    "Look at this example:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/composing_network.png\" alt=\"composing networks example\">\n",
    "</div>\n",
    "\n",
    "The space has 3 time folded because Network 1 has been moved from -1 to 1:\n",
    "- between -1, 0\n",
    "- between 0, 0.8\n",
    "- between 0.8, 1\n",
    "\n",
    "And Network 2 will replace 3 times in these boundaries\n",
    "\n",
    "**Key Insight:**\n",
    "- Each additional layer multiplies the number of linear regions.\n",
    "- For a network with $K$ layers and $n$ neurons per layer, the number of linear regions grows exponentially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f552c6",
   "metadata": {},
   "source": [
    "### **Result:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e48df5",
   "metadata": {},
   "source": [
    "Max number of linear regions increases as a function of the number of parameters for networks mapping scalar input x to scalar output y.\n",
    "\n",
    "For K layers:\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/max_number_regions.png\" alt=\"max number of regions\">\n",
    "</div>\n",
    "\n",
    "Adding more layers increases the number of linear regions **exponentially.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23db841",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
