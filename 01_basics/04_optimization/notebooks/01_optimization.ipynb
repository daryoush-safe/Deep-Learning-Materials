{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd675e7",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17918ed6",
   "metadata": {},
   "source": [
    "**Convex Functions**\n",
    "\n",
    "**Definition:**  \n",
    "A function is convex if, for any two points on or above its surface, the line segment connecting them lies entirely **above** the surface **without intersecting it**.\n",
    "\n",
    "**Key Properties:**\n",
    "- **Single global minimum** $\\rightarrow$ Easier to optimize since no local minima or saddle points exist.\n",
    "- **Numerical methods (e.g., Gradient Descent)** are guaranteed to converge to the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c87bb",
   "metadata": {},
   "source": [
    "**Non-Convex Functions**\n",
    "\n",
    "**Definition:**  \n",
    "Functions with complex surfaces that may contain multiple minima, maxima, and saddle points.\n",
    "\n",
    "**Challenges in Optimization:**\n",
    "- **No guaranteed convergence** to a global minimum.\n",
    "- Algorithms may exhibit unstable behavior:\n",
    "    - **Jitter:** Oscillations near a minimum due to high gradients or learning rates.\n",
    "        <div style=\"text-align:center\">\n",
    "        <img src=\"../assets/jitter.png\" alt=\"jittering\">\n",
    "        </div>\n",
    "    - **Diverge:** The optimization fails entirely, with the loss increasing uncontrollably.\n",
    "        <div style=\"text-align:center\">\n",
    "        <img src=\"../assets/diverging.png\" alt=\"diverging\">\n",
    "        </div>\n",
    "\n",
    "**Critical Points in Non-Convex Functions:**\n",
    "- **Global minimum**\n",
    "    - Overall lowest point\n",
    "- **Local minimum**\n",
    "    -  Lower than nearby points, but not the lowest overall.\n",
    "- **Saddle points:**\n",
    "    - Regions where the **gradient is zero** but can **increase or decrease** in other directions.\n",
    "    - Some of the **Eigenvalues** of the **Hessian** are positive; others are negative\n",
    "\n",
    "*Neural network loss surfaces are typically **non-convex**, making optimization challenging.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b3ddd7",
   "metadata": {},
   "source": [
    "**The Controversial Error Surface**\n",
    "- An **exponential number of saddle** points in **large networks** (Dauphin et. al (2015))\n",
    "- For **large networks**, most **local minima** lie in a band and are **equivalent** (Chomoranksa et. al (2015))\n",
    "- In **networks of small size**, trained on finite data, you can have **horrible local minima** (Swirscz et. al. (2016))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0465da5f",
   "metadata": {},
   "source": [
    "# Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21eb711",
   "metadata": {},
   "source": [
    "## Newton's Original Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf61a7f7",
   "metadata": {},
   "source": [
    "**Objective**  \n",
    "Newton's method is derived by finding the tangent line of a function $ f(x) $ at an initial point $ x_0 $.\n",
    "\n",
    "**Tangent Line Equation**  \n",
    "Given a point $ x_0 $ where $ f(x_0) \\neq 0 $, the tangent line at $ x_0 $ is expressed as:\n",
    "$$ y = mx_0 + c $$\n",
    "\n",
    "- **Gradient (Slope):**  \n",
    "  The slope $ m $ is equal to the derivative of $ f(x) $ evaluated at $ x_0 $:\n",
    "  $$ m = f'(x_0) $$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/newton_method.png\" alt=\"Newton's method\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa72294f",
   "metadata": {},
   "source": [
    "**Finding the Y-Intercept $ c $**  \n",
    "Substitute the point $ (x_0, f(x_0)) $ into the tangent line equation $ y = mx + c $:\n",
    "$$ f(x_0) = f'(x_0)x_0 + c $$  \n",
    "Solving for $ c $:\n",
    "$$ c = f(x_0) - f'(x_0)x_0 $$\n",
    "\n",
    "**Final Tangent Line Equation**  \n",
    "Substitute $ m = f'(x_0) $ and $ c $ back into the tangent line equation:\n",
    "$$ y = f'(x_0)x + f(x_0) - f'(x_0)x_0 $$  \n",
    "Simplify to obtain the standard form:\n",
    "$$ y = f(x_0) + f'(x_0)(x - x_0) $$\n",
    "\n",
    "**Approximating the Root**  \n",
    "To approximate the root of $ f(x) $, set $ y = 0 $ in the tangent line equation:\n",
    "$$ 0 = f(x_0) + f'(x_0)(x_1 - x_0) $$  \n",
    "Rearrange to solve for the next approximation $ x_1 $:\n",
    "$$ x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)} $$\n",
    "\n",
    "**Iteration Process**  \n",
    "Repeat the step above to iteratively refine the approximation of the root:\n",
    "$$ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f91c8",
   "metadata": {},
   "source": [
    "## Newtonâ€™s Method for Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9458d83b",
   "metadata": {},
   "source": [
    "**From Root-Finding to Optimization**\n",
    "\n",
    "- **Root-finding (Original Newton's Method):**\n",
    "    - Uses a **first-order approximation** (tangent line) to iteratively find roots of $f(w)$.\n",
    "\n",
    "- **Optimization Adaptation:**\n",
    "    - Uses a **second-order Taylor approximation** to locate minima/maxima of a function $E(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fa216a",
   "metadata": {},
   "source": [
    "**Second-order Taylor expansion** of $E(w)$ around $w = w_k$:\n",
    "$$\n",
    "E(w) \\approx E(w^{(k)}) + E'(w^{(k)})(w - w^{(k)}) + \\frac{1}{2} E''(w^{(k)}) (w - w^{(k)})^2\n",
    "$$\n",
    "**Key Property:** Exact equality holds if $E(w)$ is **quadratic**.\n",
    "\n",
    "**Optimality Condition:**  \n",
    "To find $\\hat{w} = \\argmin_w E(w)$, set the gradient of the approximation to zero:\n",
    "$$\n",
    "\\frac{\\partial E(w)}{\\partial w} = E'(w^{(k)}) + E''(w^{(k)}) (w - w^{(k)}) = 0\n",
    "$$\n",
    "\n",
    "Multiply through by the inverse Hessian $E''(w^{(k)})^{-1}$:\n",
    "$$\n",
    "E''(w^{(k)})^{-1} E'(w^{(k)}) + w - w^{(k)} = 0\n",
    "$$\n",
    "\n",
    "Rearrange to obtain the **Newton update rule**:\n",
    "$$\n",
    "w = w^{(k)} - E''(w^{(k)})^{-1} E'(w^{(k)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e184d",
   "metadata": {},
   "source": [
    "**Hessian Matrix $H(\\theta)$**\n",
    "\n",
    "Given a scalar-valued function $ f(\\theta) $, where  \n",
    "$ \\theta = [\\theta_1, \\theta_2, \\dots, \\theta_n]^T $,  \n",
    "the **Hessian matrix** $ H(\\theta) $ is the square matrix of second-order partial derivatives of scalar-valued function:\n",
    "\n",
    "$$\n",
    "H(\\theta) = \\nabla^2 f(\\theta) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial \\theta_1^2} & \\frac{\\partial^2 f}{\\partial \\theta_1 \\partial \\theta_2} & \\cdots & \\frac{\\partial^2 f}{\\partial \\theta_1 \\partial \\theta_n} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial \\theta_2 \\partial \\theta_1} & \\frac{\\partial^2 f}{\\partial \\theta_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial \\theta_2 \\partial \\theta_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial^2 f}{\\partial \\theta_n \\partial \\theta_1} & \\frac{\\partial^2 f}{\\partial \\theta_n \\partial \\theta_2} & \\cdots & \\frac{\\partial^2 f}{\\partial \\theta_n^2}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76eea9",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- $E''(w^{(k)})$: $H_E(w^{(k)})$ Hessian matrix\n",
    "- $E'(w^{(k)})$: $\\nabla_w E(w^{(k)})$ Gradient matrix\n",
    "\n",
    "Substitute in **Newton update rule**\n",
    "$$\n",
    "E(w) \\approx E(w^{(k)}) + \\nabla_w E(w^{(k)}) (w - w^{(k)}) + \\frac{1}{2} H_E(w^{(k)}) (w - w^{(k)})^2\n",
    "$$\n",
    "Result is:\n",
    "$$\n",
    "w = w^{(k)} - H_E(w^{(k)})^{-1} \\nabla_w E(w^{(k)})\n",
    "$$\n",
    "\n",
    "We can arrive at the optimum in a single step using the optimum step size:\n",
    "$$\n",
    "\\eta_{opt} = E''(w^{(k)})^{-1} = H_E(w^{(k)})^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c8287",
   "metadata": {},
   "source": [
    "**With Non-Optimal Step Size**\n",
    "\n",
    "Gradient descent with fixed step size $\\eta$ to estimate scalar parameter $w$:\n",
    "$$\n",
    "W^{(k + 1)} = w^{(k)} - \\eta \\frac{\\partial E(w^{(k)})}{\\partial w}\n",
    "$$\n",
    "\n",
    "- For $\\eta \\lt \\eta_{opt}$ the algorithm will converge monotonically\n",
    "\n",
    "- For $2\\eta_{opt} \\gt \\eta \\gt \\eta_{opt}$ we have oscillating convergence\n",
    "\n",
    "- For $\\eta \\gt 2\\eta_{opt}$ we get divergence\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/non_optimal_step_size.png\" alt=\"non-optimal step size\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eddf4ce",
   "metadata": {},
   "source": [
    "## Advantages & Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b3341",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "\n",
    "- **Faster Convergence**\n",
    "    - Quadratic convergence enables reaching minima faster in convex problems.\n",
    "\n",
    "- **Adaptive Step Sizes**\n",
    "    - Curvature-based step adjustment avoids slow progress in shallow regions.\n",
    "\n",
    "- **Reduced Oscillations**\n",
    "    - Curvature information stabilizes paths in oscillatory regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0bb2f",
   "metadata": {},
   "source": [
    "**Disadvantages**\n",
    "\n",
    "- **Computationally Expensive**\n",
    "    - Requires Hessian calculation, making it costly in high-dimensional models.\n",
    "\n",
    "- **Memory Intensive**\n",
    "    - Storing the Hessian matrix is memory-intensive for models with millions of parameters.\n",
    "\n",
    "- **Convergence Challenges**\n",
    "    - May converge to saddle points in non-convex functions common in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b507f50",
   "metadata": {},
   "source": [
    "## Remarks on Second-Order Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e2ae3",
   "metadata": {},
   "source": [
    "**Purpose:**  \n",
    "Normalize gradient updates across different directions to handle varying curvature, eliminating the need for per-component learning rate tuning.\n",
    "\n",
    "**Challenges:**\n",
    "- Requires computing (and inverting) second-derivative matrices (Hessians), which is **computationally infeasible** for large models.\n",
    "- Unstable in **non-convex regions** (e.g., saddle points, sharp minima).\n",
    "\n",
    "**Workarounds:**\n",
    "- Approximate methods (e.g., **Quasi-Newton, L-BFGS**) mitigate these issues but may still be less practical than first-order methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63459d3b",
   "metadata": {},
   "source": [
    "**Non-Convex Optimization in Neural Networks**\n",
    "\n",
    "**Learning Rate Dynamics:**\n",
    "- A learning rate $\\eta > 2\\eta_{opt}$ can help escape poor local optima by overshooting shallow minima.\n",
    "- However, persistently using $\\eta > 2\\eta_{opt}$ prevents convergence entirely (divergence or oscillations).\n",
    "\n",
    "**Practical Solutions:**\n",
    "- **Decaying Learning Rates:**\n",
    "    - Start with a higher $\\eta$ to escape bad minima, then reduce it for stable convergence.\n",
    "- **Adaptive Methods (e.g., Adam, RMSprop):**\n",
    "    - Automatically adjust $\\eta$ per parameter, balancing exploration and convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1584ce",
   "metadata": {},
   "source": [
    "# Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6a7a0",
   "metadata": {},
   "source": [
    "## First Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22deb108",
   "metadata": {},
   "source": [
    "### **Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f26fa",
   "metadata": {},
   "source": [
    "**Definition:**  \n",
    "The first moment, $m_t$, represents an **exponentially weighted moving average of past gradients**, acting as a **velocity** term in parameter updates.\n",
    "\n",
    "**Physical Interpretation:**  \n",
    "- **Damps oscillations** in steep directions where gradients frequently change sign\n",
    "- **Accelerates convergence** in stable descent directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd364f1",
   "metadata": {},
   "source": [
    "### **Update Rule**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ded1b",
   "metadata": {},
   "source": [
    "For iteration $k$:\n",
    "$$m^{(k)} = \\beta_1 m^{(k-1)} + (1 - \\beta_1) \\nabla_w E(W^{(k-1)})$$\n",
    "$$W^{(k)} = W^{(k-1)} - \\eta m^{(k)}$$\n",
    "where:\n",
    "- $\\beta_1$: Decay rate, usually 0.9 or 0.99, which controls the weight of past gradients.\n",
    "- $\\eta$: Learning rate\n",
    "\n",
    "**Key Properties:**\n",
    "- $m^{(k)}$: Momentum\n",
    "- $\\beta_1$: Less than 1\n",
    "- $\\beta_1 m^{(k-1)}$: Exponentially weighted averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d247e5",
   "metadata": {},
   "source": [
    "### **Behavior Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b38ac72",
   "metadata": {},
   "source": [
    "**Consistent Gradient Directions:**\n",
    "- Velocity builds up $\\rightarrow$ longer effective steps\n",
    "\n",
    "**Oscillating Gradients:**\n",
    "- Opposing forces cancel out $\\rightarrow$ shorter steps\n",
    "- Provides natural oscillation damping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7055b9cc",
   "metadata": {},
   "source": [
    "### **Exponentially Weighted Averages**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d4afc8",
   "metadata": {},
   "source": [
    "**Initialize:**\n",
    "$$v_0 = 0$$\n",
    "\n",
    "**Recursive Update:**\n",
    "$$v_t = \\beta v_{t-1} + \\alpha_t$$\n",
    "\n",
    "**Expanded Form:**\n",
    "$$\n",
    "v_t = \\alpha_t + \\beta \\alpha_{t-1} + \\beta^2 \\alpha_{t-2} + \\cdots + \\beta^{t-1} \\alpha_{1}\n",
    "$$\n",
    "\n",
    "**Memory Characteristics:**\n",
    "- Since $\\beta \\lt 1$, weight $\\beta^k$ decreases exponentially with $k$\n",
    "- Older gradients have diminishing influence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8096a5",
   "metadata": {},
   "source": [
    "**Practical Implications:**\n",
    "- $\\beta$ close to 1:\n",
    "    - Smoother momentum but slower to adapt\n",
    "\n",
    "- $\\beta$ too low:\n",
    "    - More responsive but less noise reduction\n",
    "\n",
    "- Optimal values:\n",
    "    - Typically $\\beta_1 \\in [0.9, 0.99]$ in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f95823",
   "metadata": {},
   "source": [
    "## Nestorov's Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e582dac0",
   "metadata": {},
   "source": [
    "Change the order of operations:\n",
    "\n",
    "At any iteration, to compute the current step:\n",
    "- First extend the previous step\n",
    "- Then compute the gradient step at the resultant position\n",
    "- Add the two to obtain the final step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3568a46e",
   "metadata": {},
   "source": [
    "**Nestorov's Update Rule:**\n",
    "\n",
    "For iteration $k$:\n",
    "$$m^{(k)} = \\beta_1 m^{(k-1)} + (1 - \\beta_1) \\nabla_w E(W^{(k-1)} + \\beta_1 m^{(k-1)})$$\n",
    "$$W^{(k)} = W^{(k-1)} - \\eta m^{(k)}$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/momentum_vs_nesterov.png\" alt=\"Momentum vs. Nesterov\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158841f5",
   "metadata": {},
   "source": [
    "**Nesterov Momentum** is a slightly different version of the momentum update\n",
    "- It uses a **look-ahead gradient step**\n",
    "- Works better than regular momentum, especially for smooth (convex) problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b298d885",
   "metadata": {},
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb50fb",
   "metadata": {},
   "source": [
    "**Objective:**  \n",
    "Downscale a model parameter by square-root of sum of squares of all its historical derivatives\n",
    "\n",
    "**Key Components:**\n",
    "- $(\\partial_w E)^{(k)}$: Gradient of loss w.r.t parameter $w$ at step $k$\n",
    "- $s^{(k)}$: Accumulated sum of squared gradients\n",
    "- $\\epsilon$: Small smoothing constant (typically 1e-8) to prevent division by zero\n",
    "\n",
    "**Update Rule:**\n",
    "$$s^{(k)} = \\beta s^{(k - 1)} + (1 - \\beta) {(\\partial_w E)^2}^{(k)}$$\n",
    "Parameter update with **adaptive** learning rate:\n",
    "$$w^{(k + 1)} = w^{(k)} - \\frac{\\eta}{\\sqrt{s^{(k)} + \\epsilon}} (\\partial_w E)^{(k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac4fff",
   "metadata": {},
   "source": [
    "**Critical Limitation - Vanishing Learning Rates:**\n",
    "\n",
    "- As $s^{(k)}$ grows monotonically, the effective learning rate $\\frac{\\eta}{\\sqrt{s^{(k)}}}$ decays to zero\n",
    "\n",
    "- Causes premature stop in learning:\n",
    "    - Early in training: Large, effective updates\n",
    "    - Later in training: Updates become infinitesimally small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79800aae",
   "metadata": {},
   "source": [
    "## RMS Prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48921e86",
   "metadata": {},
   "source": [
    "**Modified update rule: We want to**\n",
    "- **scale down** updates with **large** mean squared derivatives\n",
    "- **scale up** updates with **small** mean squared derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b109bfa",
   "metadata": {},
   "source": [
    "**Procedure:**\n",
    "- Maintain a **running estimate of the mean squared** value of derivatives for each parameter\n",
    "- Scale update of the parameter by the inverse of the **root mean squared** elements of gradient\n",
    "\n",
    "**Key Components:**\n",
    "- $(\\partial_w E)^{(k)}$: Gradient of loss w.r.t parameter $w$ at step $k$\n",
    "\n",
    "- $v^{(k)} = \\bigl(\\overline{\\partial_w E}\\bigr)^{2\\,(k)}$: Moving average pf squared gradients\n",
    "\n",
    "\n",
    "- $\\epsilon$: Small smoothing constant (typically 1e-8) to prevent division by zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de9fde6",
   "metadata": {},
   "source": [
    "**Update Rule:**\n",
    "$$v^{(k)} = \\beta_2 v^{(k - 1)} + (1 - \\beta_2) {(\\partial_w E)^2}^{(k)}$$\n",
    "Parameter update with **adaptive** learning rate:\n",
    "$$w^{(k + 1)} = w^{(k)} - \\frac{\\eta}{\\sqrt{v^{(k)} + \\epsilon}} (\\partial_w E)^{(k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de7fea9",
   "metadata": {},
   "source": [
    "**Why Use Second Momentum?**\n",
    "- Adjusts **step size** based on gradient magnitude\n",
    "    - Preventing large steps when gradients are large\n",
    "    - Accelerating learning when they are small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e928d1",
   "metadata": {},
   "source": [
    "## ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad2923",
   "metadata": {},
   "source": [
    "### **Moment Bias Correction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64950e",
   "metadata": {},
   "source": [
    "**Problem:**  \n",
    "When we start training, both $m_t$ and $v_t$ are initialized to zero, causing their estimates to be biased toward zero in the early steps, especially when gradients are small.\n",
    "\n",
    "**Solution:**  \n",
    "We use bias-corrected versions of $m_t$ and $v_t$ to address this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367443f6",
   "metadata": {},
   "source": [
    "**Initialization:**\n",
    "$$v_0 = 0$$\n",
    "\n",
    "**Recursive Update:**\n",
    "$$v_t = \\gamma v_{t-1} + (1 - \\gamma) \\theta_t$$\n",
    "where:\n",
    "- $\\gamma \\in [0,1)$: Decay rate (controls memory length)\n",
    "- $\\theta_t$: Current observation\n",
    "\n",
    "**Expanded Form:**\n",
    "$$v_t = (1 - \\gamma)(\\theta_t + \\gamma \\theta_{t-1} + \\gamma^2 \\theta_{t-2} + \\cdots + \\gamma^{t - 1} \\theta_1)$$\n",
    "\n",
    "**Geometric Series Property:**\n",
    "$$1 - \\gamma^t = (1 - \\gamma) (1 + \\gamma + \\gamma^2 + \\cdots + \\gamma^{t - 1})$$\n",
    "\n",
    "**Bias-Corrected Average:**\n",
    "$$\\frac{v_t}{1 - \\gamma^t} = \\frac{\\theta_t + \\gamma \\theta_{t-1} + \\gamma^2 \\theta_{t-2} + \\cdots + \\gamma^{t - 1} \\theta_1}{1 + \\gamma + \\gamma^2 + \\cdots + \\gamma^{t - 1}}$$\n",
    "\n",
    "**Key Properties:**\n",
    "- **Weight Sum:** Normalized to 1 (convex combination)\n",
    "- **Bias Correction:** Crucial for early steps when $t$ is small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af29e57",
   "metadata": {},
   "source": [
    "### **Adam's Adaptive Learning Rate Mechanism**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab97e410",
   "metadata": {},
   "source": [
    "**Why Adaptive Rates?**\n",
    "- Unlike traditional SGD, Adam adapts the learning rate for **each parameter** based on recent gradient magnitudes.\n",
    "- **Large gradients** lead to reduced update sizes, while **smaller** gradients allow **larger** updates, balancing convergence speed and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe68dfb",
   "metadata": {},
   "source": [
    "**Moment Tracking**\n",
    "- The **first moment (mt)** tracks the mean of gradients to provide momentum.\n",
    "- The **second moment (vt)** tracks squared gradients, enabling Adam to normalize updates and prevent sudden changes in direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c1440",
   "metadata": {},
   "source": [
    "**Adam Update Rules:**\n",
    "- **First Moment Estimate:**\n",
    "$$m_{t+1} = \\beta_1 m_t + (1 - \\beta_1) \\nabla_w J(w_t)$$\n",
    "\n",
    "- **Second Moment Estimate:**\n",
    "$$v_{t+1} = \\beta_2 v_t + (1 - \\beta_2)(\\nabla_w J(w_t))^2$$\n",
    "\n",
    "- Bias-corrected moments to address initialization bias:\n",
    "$$\\hat{m_{t+1}} = \\frac{m_{t+1}}{1 - {\\beta_1}^{t+1}}, \\quad \\hat{v_{t+1}} = \\frac{v_{t+1}}{1 - {\\beta_2}^{t+1}}$$\n",
    "\n",
    "- Update step for parameter $w_t$:\n",
    "$$w_{t+1} = w_t - \\eta \\frac{\\hat{m}_{t+1}}{\\sqrt{\\hat{v_t} + \\epsilon}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b579b80b",
   "metadata": {},
   "source": [
    "### **Adam Pseudocode**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ed1976",
   "metadata": {},
   "source": [
    ">**Algorithm 1**: *Adam*, our proposed algorithm for stochastic optimization. See section 2 for details,  \n",
    ">and for a slightly more efficient (but less clear) order of computation. $ g_t^2 $ indicates the elementwise  \n",
    ">square $ g_t \\odot g_t $. Good default settings for the tested machine learning problems are  \n",
    ">$ \\alpha = 0.001 $, $ \\beta_1 = 0.9 $, $ \\beta_2 = 0.999 $ and $ \\epsilon = 10^{-8} $. All operations on vectors are element-wise.  \n",
    ">With $ \\beta_1^t $ and $ \\beta_2^t $ we denote $ \\beta_1 $ and $ \\beta_2 $ to the power $ t $.\n",
    ">**Require**: $ \\alpha $: Stepsize  \n",
    ">**Require**: $ \\beta_1, \\beta_2 \\in [0, 1) $: Exponential decay rates for the moment estimates  \n",
    ">**Require**: $ f(\\theta) $: Stochastic objective function with parameters $ \\theta $  \n",
    ">**Require**: $ \\theta_0 $: Initial parameter vector  \n",
    ">$ m_0 \\leftarrow 0 $ (Initialize 1st moment vector)  \n",
    ">$ v_0 \\leftarrow 0 $ (Initialize 2nd moment vector)  \n",
    ">$ t \\leftarrow 0 $ (Initialize timestep)  \n",
    ">**while** $ \\theta_t $ not converged **do**  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$ t \\leftarrow t + 1 $  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$ g_t \\leftarrow \\nabla_\\theta f_t(\\theta_{t-1}) $ (Get gradients w.r.t. stochastic objective at timestep $ t $)  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$ m_t \\leftarrow \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t $ (Update biased first moment estimate)  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$ v_t \\leftarrow \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2 $ (Update biased second raw moment estimate)  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$ \\hat{m}_t \\leftarrow m_t / (1 - \\beta_1^t) $ (Compute bias-corrected first moment estimate)  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$ \\hat{v}_t \\leftarrow v_t / (1 - \\beta_2^t) $ (Compute bias-corrected second raw moment estimate)  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;$ \\theta_t \\leftarrow \\theta_{t-1} - \\alpha \\cdot \\hat{m}_t / (\\sqrt{\\hat{v}_t} + \\epsilon) $ (Update parameters)  \n",
    ">**end while**  \n",
    ">**return** $ \\theta_t $ (Resulting parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eca9975",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
