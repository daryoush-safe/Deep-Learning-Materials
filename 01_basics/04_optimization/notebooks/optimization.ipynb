{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd675e7",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17918ed6",
   "metadata": {},
   "source": [
    "**Convex Functions**\n",
    "\n",
    "**Definition:**  \n",
    "A function is convex if, for any two points on or above its surface, the line segment connecting them lies entirely **above** the surface **without intersecting it**.\n",
    "\n",
    "**Key Properties:**\n",
    "- **Single global minimum** $\\rightarrow$ Easier to optimize since no local minima or saddle points exist.\n",
    "- **Numerical methods (e.g., Gradient Descent)** are guaranteed to converge to the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c87bb",
   "metadata": {},
   "source": [
    "**Non-Convex Functions**\n",
    "\n",
    "**Definition:**  \n",
    "Functions with complex surfaces that may contain multiple minima, maxima, and saddle points.\n",
    "\n",
    "**Challenges in Optimization:**\n",
    "- **No guaranteed convergence** to a global minimum.\n",
    "- Algorithms may exhibit unstable behavior:\n",
    "    - **Jitter:** Oscillations near a minimum due to high gradients or learning rates.\n",
    "        <div style=\"text-align:center\">\n",
    "        <img src=\"../assets/jitter.png\" alt=\"jittering\">\n",
    "        </div>\n",
    "    - **Diverge:** The optimization fails entirely, with the loss increasing uncontrollably.\n",
    "        <div style=\"text-align:center\">\n",
    "        <img src=\"../assets/diverging.png\" alt=\"diverging\">\n",
    "        </div>\n",
    "\n",
    "**Critical Points in Non-Convex Functions:**\n",
    "- **Global minimum**\n",
    "    - Overall lowest point\n",
    "- **Local minimum**\n",
    "    -  Lower than nearby points, but not the lowest overall.\n",
    "- **Saddle points:**\n",
    "    - Regions where the **gradient is zero** but can **increase or decrease** in other directions.\n",
    "    - Some of the **Eigenvalues** of the **Hessian** are positive; others are negative\n",
    "\n",
    "*Neural network loss surfaces are typically **non-convex**, making optimization challenging.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b3ddd7",
   "metadata": {},
   "source": [
    "**The Controversial Error Surface**\n",
    "- An **exponential number of saddle** points in **large networks** (Dauphin et. al (2015))\n",
    "- For **large networks**, most **local minima** lie in a band and are **equivalent** (Chomoranksa et. al (2015))\n",
    "- In **networks of small size**, trained on finite data, you can have **horrible local minima** (Swirscz et. al. (2016))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0465da5f",
   "metadata": {},
   "source": [
    "# Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21eb711",
   "metadata": {},
   "source": [
    "## Newton's Original Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf61a7f7",
   "metadata": {},
   "source": [
    "**Objective**  \n",
    "Newton's method is derived by finding the tangent line of a function $ f(x) $ at an initial point $ x_0 $.\n",
    "\n",
    "**Tangent Line Equation**  \n",
    "Given a point $ x_0 $ where $ f(x_0) \\neq 0 $, the tangent line at $ x_0 $ is expressed as:\n",
    "$$ y = mx_0 + c $$\n",
    "\n",
    "- **Gradient (Slope):**  \n",
    "  The slope $ m $ is equal to the derivative of $ f(x) $ evaluated at $ x_0 $:\n",
    "  $$ m = f'(x_0) $$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/newton_method.png\" alt=\"Newton's method\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa72294f",
   "metadata": {},
   "source": [
    "**Finding the Y-Intercept $ c $**  \n",
    "Substitute the point $ (x_0, f(x_0)) $ into the tangent line equation $ y = mx + c $:\n",
    "$$ f(x_0) = f'(x_0)x_0 + c $$  \n",
    "Solving for $ c $:\n",
    "$$ c = f(x_0) - f'(x_0)x_0 $$\n",
    "\n",
    "**Final Tangent Line Equation**  \n",
    "Substitute $ m = f'(x_0) $ and $ c $ back into the tangent line equation:\n",
    "$$ y = f'(x_0)x + f(x_0) - f'(x_0)x_0 $$  \n",
    "Simplify to obtain the standard form:\n",
    "$$ y = f(x_0) + f'(x_0)(x - x_0) $$\n",
    "\n",
    "**Approximating the Root**  \n",
    "To approximate the root of $ f(x) $, set $ y = 0 $ in the tangent line equation:\n",
    "$$ 0 = f(x_0) + f'(x_0)(x_1 - x_0) $$  \n",
    "Rearrange to solve for the next approximation $ x_1 $:\n",
    "$$ x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)} $$\n",
    "\n",
    "**Iteration Process**  \n",
    "Repeat the step above to iteratively refine the approximation of the root:\n",
    "$$ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f91c8",
   "metadata": {},
   "source": [
    "## Newtonâ€™s Method for Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9458d83b",
   "metadata": {},
   "source": [
    "**From Root-Finding to Optimization**\n",
    "\n",
    "- **Root-finding (Original Newton's Method):**\n",
    "    - Uses a **first-order approximation** (tangent line) to iteratively find roots of $f(w)$.\n",
    "\n",
    "- **Optimization Adaptation:**\n",
    "    - Uses a **second-order Taylor approximation** to locate minima/maxima of a function $E(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fa216a",
   "metadata": {},
   "source": [
    "**Second-order Taylor expansion** of $E(w)$ around $w = w_k$:\n",
    "$$\n",
    "E(w) \\approx E(w^{(k)}) + E'(w^{(k)})(w - w^{(k)}) + \\frac{1}{2} E''(w^{(k)}) (w - w^{(k)})^2\n",
    "$$\n",
    "**Key Property:** Exact equality holds if $E(w)$ is **quadratic**.\n",
    "\n",
    "**Optimality Condition:**  \n",
    "To find $\\hat{w} = \\argmin_w E(w)$, set the gradient of the approximation to zero:\n",
    "$$\n",
    "\\frac{\\partial E(w)}{\\partial w} = E'(w^{(k)}) + E''(w^{(k)}) (w - w^{(k)}) = 0\n",
    "$$\n",
    "\n",
    "Multiply through by the inverse Hessian $E''(w^{(k)})^{-1}$:\n",
    "$$\n",
    "E''(w^{(k)})^{-1} E'(w^{(k)}) + w - w^{(k)} = 0\n",
    "$$\n",
    "\n",
    "Rearrange to obtain the **Newton update rule**:\n",
    "$$\n",
    "w = w^{(k)} - E''(w^{(k)})^{-1} E'(w^{(k)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e184d",
   "metadata": {},
   "source": [
    "**Hessian Matrix $H(\\theta)$**\n",
    "\n",
    "Given a scalar-valued function $ f(\\theta) $, where  \n",
    "$ \\theta = [\\theta_1, \\theta_2, \\dots, \\theta_n]^T $,  \n",
    "the **Hessian matrix** $ H(\\theta) $ is the square matrix of second-order partial derivatives of scalar-valued function:\n",
    "\n",
    "$$\n",
    "H(\\theta) = \\nabla^2 f(\\theta) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial \\theta_1^2} & \\frac{\\partial^2 f}{\\partial \\theta_1 \\partial \\theta_2} & \\cdots & \\frac{\\partial^2 f}{\\partial \\theta_1 \\partial \\theta_n} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial \\theta_2 \\partial \\theta_1} & \\frac{\\partial^2 f}{\\partial \\theta_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial \\theta_2 \\partial \\theta_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial^2 f}{\\partial \\theta_n \\partial \\theta_1} & \\frac{\\partial^2 f}{\\partial \\theta_n \\partial \\theta_2} & \\cdots & \\frac{\\partial^2 f}{\\partial \\theta_n^2}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76eea9",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- $E''(w^{(k)})$: $H_E(w^{(k)})$ Hessian matrix\n",
    "- $E'(w^{(k)})$: $\\nabla_w E(w^{(k)})$ Gradient matrix\n",
    "\n",
    "Substitute in **Newton update rule**\n",
    "$$\n",
    "E(w) \\approx E(w^{(k)}) + \\nabla_w E(w^{(k)}) (w - w^{(k)}) + \\frac{1}{2} H_E(w^{(k)}) (w - w^{(k)})^2\n",
    "$$\n",
    "Result is:\n",
    "$$\n",
    "w = w^{(k)} - H_E(w^{(k)})^{-1} \\nabla_w E(w^{(k)})\n",
    "$$\n",
    "\n",
    "We can arrive at the optimum in a single step using the optimum step size:\n",
    "$$\n",
    "\\eta_{opt} = E''(w^{(k)})^{-1} = H_E(w^{(k)})^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c8287",
   "metadata": {},
   "source": [
    "**With Non-Optimal Step Size**\n",
    "\n",
    "Gradient descent with fixed step size $\\eta$ to estimate scalar parameter $w$:\n",
    "$$\n",
    "W^{(k + 1)} = w^{(k)} - \\eta \\frac{\\partial E(w^{(k)})}{\\partial w}\n",
    "$$\n",
    "\n",
    "- For $\\eta \\lt \\eta_{opt}$ the algorithm will converge monotonically\n",
    "\n",
    "- For $2\\eta_{opt} \\gt \\eta \\gt \\eta_{opt}$ we have oscillating convergence\n",
    "\n",
    "- For $\\eta \\gt 2\\eta_{opt}$ we get divergence\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/non_optimal_step_size.png\" alt=\"non-optimal step size\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eddf4ce",
   "metadata": {},
   "source": [
    "## Advantages & Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b3341",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "\n",
    "- **Faster Convergence**\n",
    "    - Quadratic convergence enables reaching minima faster in convex problems.\n",
    "\n",
    "- **Adaptive Step Sizes**\n",
    "    - Curvature-based step adjustment avoids slow progress in shallow regions.\n",
    "\n",
    "- **Reduced Oscillations**\n",
    "    - Curvature information stabilizes paths in oscillatory regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0bb2f",
   "metadata": {},
   "source": [
    "**Disadvantages**\n",
    "\n",
    "- **Computationally Expensive**\n",
    "    - Requires Hessian calculation, making it costly in high-dimensional models.\n",
    "\n",
    "- **Memory Intensive**\n",
    "    - Storing the Hessian matrix is memory-intensive for models with millions of parameters.\n",
    "\n",
    "- **Convergence Challenges**\n",
    "    - May converge to saddle points in non-convex functions common in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b507f50",
   "metadata": {},
   "source": [
    "## Remarks on Second-Order Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e2ae3",
   "metadata": {},
   "source": [
    "**Purpose:**  \n",
    "Normalize gradient updates across different directions to handle varying curvature, eliminating the need for per-component learning rate tuning.\n",
    "\n",
    "**Challenges:**\n",
    "- Requires computing (and inverting) second-derivative matrices (Hessians), which is **computationally infeasible** for large models.\n",
    "- Unstable in **non-convex regions** (e.g., saddle points, sharp minima).\n",
    "\n",
    "**Workarounds:**\n",
    "- Approximate methods (e.g., **Quasi-Newton, L-BFGS**) mitigate these issues but may still be less practical than first-order methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63459d3b",
   "metadata": {},
   "source": [
    "**Non-Convex Optimization in Neural Networks**\n",
    "\n",
    "**Learning Rate Dynamics:**\n",
    "- A learning rate $\\eta > 2\\eta_{opt}$ can help escape poor local optima by overshooting shallow minima.\n",
    "- However, persistently using $\\eta > 2\\eta_{opt}$ prevents convergence entirely (divergence or oscillations).\n",
    "\n",
    "**Practical Solutions:**\n",
    "- **Decaying Learning Rates:**\n",
    "    - Start with a higher $\\eta$ to escape bad minima, then reduce it for stable convergence.\n",
    "- **Adaptive Methods (e.g., Adam, RMSprop):**\n",
    "    - Automatically adjust $\\eta$ per parameter, balancing exploration and convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1584ce",
   "metadata": {},
   "source": [
    "# Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6a7a0",
   "metadata": {},
   "source": [
    "## First Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f95823",
   "metadata": {},
   "source": [
    "## Nestorov's Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b298d885",
   "metadata": {},
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79800aae",
   "metadata": {},
   "source": [
    "## RMS Prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e928d1",
   "metadata": {},
   "source": [
    "## ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad2923",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
