{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb42c71",
   "metadata": {},
   "source": [
    "# Overview of Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b876acb4",
   "metadata": {},
   "source": [
    "Gradient descent is a **first-order iterative** optimization algorithm for finding the minimum of a function.  \n",
    "It's used to minimize the loss function by updating model parameters in the **opposite direction of the gradient**.\n",
    "\n",
    "**Key Components:**\n",
    "- **Learning rate ($\\eta$):** Determines step size\n",
    "- **Gradient ($\\nabla_w$):** Partial derivatives of loss w.r.t. parameters\n",
    "- **Convergence criteria:** Stopping conditions for the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2305c3",
   "metadata": {},
   "source": [
    "**Gradient Descent Variations**\n",
    "\n",
    "* **Batch Gradient Descent**:\n",
    "    - Process the **entire training set** per iteration.\n",
    "    - Provides **smooth updates** toward the optimal solution.\n",
    "    - **Computationally intensive** for large datasets.\n",
    "    - **Infeasible** for some applications. *(BGD needs entire training set which is not available in some cases like online learning)*\n",
    "\n",
    "* **Mini-batch Gradient Descent**:\n",
    "    - Process **small, random subset** of data per iteration.\n",
    "    - **Balance between efficiency and computational power** (less noisy than SGD, faster than BGD).\n",
    "    - **Preferred in practice** for most deep learning applications.\n",
    "\n",
    "* **Stochastic Gradient Descent**:\n",
    "    - Processes **one training example** per iteration.\n",
    "    - **Advantages:**\n",
    "        - Suitable for **online/real-time** learning.\n",
    "        - **Frequent updates** can lead to faster convergence.\n",
    "    - **Disadvantages:**\n",
    "        - Introduces **high variance (noise)** in updates.\n",
    "        - **Erratic convergence path** (less smooth than BGD).\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/gradient_variations.png\" alt=\"comparison of different gradient variations\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c7f75",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent (Vanilla GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe1f14",
   "metadata": {},
   "source": [
    "## Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1869723",
   "metadata": {},
   "source": [
    "- Processes the entire training set to compute a single update\n",
    "- Provides the exact gradient direction (no sampling noise)\n",
    "- Guaranteed convergence to global minimum for convex functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c363a6",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fc2be2",
   "metadata": {},
   "source": [
    "**Input:** $ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(N)}, y^{(N)}) $\n",
    "\n",
    "> Initialize all weights  \n",
    "> Do:  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update: $ W = W - \\eta_j \\text{loss}(x^{(n)}, y^{(n)}) $  \n",
    "> Until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a8ae68",
   "metadata": {},
   "source": [
    "## Advantages & Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1323b",
   "metadata": {},
   "source": [
    "**Advantages:**\n",
    "- Stable, deterministic convergence\n",
    "- Straightforward implementation\n",
    "\n",
    "**Disadvantages:**\n",
    "- Computationally expensive for large $N$\n",
    "- Requires entire dataset in memory\n",
    "- Gets stuck in local minima for non-convex functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b3be1",
   "metadata": {},
   "source": [
    "**Practical Considerations:**\n",
    "- Rarely used in modern deep learning\n",
    "- May be suitable for small datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efe4349",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75818d4",
   "metadata": {},
   "source": [
    "## Characteristics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e699f",
   "metadata": {},
   "source": [
    "- Processes **one random sample** per iteration\n",
    "- **High variance** updates (noisy gradients)\n",
    "- Can **escape local minima** due to noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0963850b",
   "metadata": {},
   "source": [
    "## SGD: Incremental Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad5bf07",
   "metadata": {},
   "source": [
    "**Algorithm:**\n",
    "\n",
    "> Given $ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(N)}, y^{(N)}) $  \n",
    "> Initialize all weights  \n",
    "> $ j = 0 $  \n",
    "> Do:  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;Randomly permute data  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;For all $ n = 1 : N $:  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$ j = j + 1 $  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update: $ W = W - \\eta_j \\sum_{n=1}^N \\text{loss}(x^{(n)}, y^{(n)}) $  \n",
    "> Until $ Err $ has converged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f3282e",
   "metadata": {},
   "source": [
    "**Key Properties:**\n",
    "- Loop through the samples in the same order, lead to **cyclic behavior**\n",
    "- Provided training instances must be presented in random order *(Stochastic Gradient Descent)*\n",
    "- Usually *learning rate* reduces with $j$ making it be the function of $j$\n",
    "- The iterations can make multiple passes over the data\n",
    "- A single pass through the entire training data is called an **epoch**\n",
    "- An epoch over a training set with $N$ samples results in $N$ updates of parameters *(for single sample SGD)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f45f1a",
   "metadata": {},
   "source": [
    "## Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86b2cc4",
   "metadata": {},
   "source": [
    "SGD **converges almost surely** to a global/local minimum under:\n",
    "\n",
    "**Sufficient Conditions:**\n",
    "- **Infinite exploration:**\n",
    "    $$\\sum_k \\eta_k = \\infty$$\n",
    "- **Vanishing steps:**\n",
    "    $$\\sum_k \\eta_k^2 \\lt \\infty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740622fb",
   "metadata": {},
   "source": [
    "The fastest converging series that satisfies both above requirements is: $\\eta_k \\propto \\frac{1}{k}$\n",
    "\n",
    "**Optimal Learning Rate Schedule:**\n",
    "- For strongly convex functions: $\\eta_k \\propto \\frac{1}{k}$\n",
    "- For non-convex functions: **Heuristic tuning** is common."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005500a3",
   "metadata": {},
   "source": [
    "**Convergence Behavior:**\n",
    "- Convex loss $\\rightarrow$ Converges to the **global optimal**\n",
    "- Non-convex loss $\\rightarrow$ Converges to a **stationary point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a068f3",
   "metadata": {},
   "source": [
    "## Minimize Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a910848",
   "metadata": {},
   "source": [
    "**Expected vs. Empirical Loss**\n",
    "\n",
    "- **Expected Loss (ideal, intractable):**\n",
    "    $$E\\left[loss\\left(f(X;W), g(X)\\right)\\right] = \\int_{X} loss\\left(f(X;W), g(X)\\right) P(X) dX$$\n",
    "\n",
    "- **Empirical Loss (practical):**\n",
    "    $$Err\\left(f(X;W), g(X)\\right) = \\frac{1}{N} \\sum_{i=1}^N loss\\left(f(x^{(i)}; W), y^{(i)}\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1733c",
   "metadata": {},
   "source": [
    "The variance of the empirical error:\n",
    "$$var(Err) = \\frac{var(loss)}{N}$$\n",
    "The larger this variance, the greater the likelihood $\\hat{W}$ will differ significantly from $W^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c381a8",
   "metadata": {},
   "source": [
    "## SGD vs. BGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6374e21f",
   "metadata": {},
   "source": [
    "| Aspect               | SGD                              | Batch GD                        |\n",
    "|----------------------|----------------------------------|---------------------------------|\n",
    "| **Update Speed**     | Fast (per-sample updates)        | Slow (full-batch updates)       |\n",
    "| **Variance**         | High (noisy gradients)           | Low (smooth convergence)        |\n",
    "| **Scalability**      | Excellent for large datasets     | Limited by memory               |\n",
    "| **Memory Usage**     | Low (processes 1 sample at time) | High (stores entire dataset)    |\n",
    "| **Convergence**      | Erratic path, may oscillate      | Stable, direct path             |\n",
    "| **Online Learning**  | Yes (can update continuously)    | No (requires full dataset)      |\n",
    "| **Use Cases**        | Large datasets, streaming data   | Small datasets (<10k samples)   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bed9b7e",
   "metadata": {},
   "source": [
    "# Mini-Batch Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3613d",
   "metadata": {},
   "source": [
    "## Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb790dfd",
   "metadata": {},
   "source": [
    "**Key Characteristics**\n",
    "- Processes **small, randomly sampled batches** (typically 32–512 samples per batch).\n",
    "- Balances noise and computational efficiency.\n",
    "- The **default choice** for most deep learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db2959a",
   "metadata": {},
   "source": [
    "**Algorithm Overview:**\n",
    "\n",
    "Given $ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(N)}, y^{(N)}) $\n",
    "\n",
    "> Initialize all weights  \n",
    "> $ j = 0 $  \n",
    "> Do:  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;Randomly permute data  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;Split data into batches $\\{B_1, \\cdots,B_m\\}$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;For each batch $B$:  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$ j = j + 1 $  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update: $ W = W - \\eta_j \\sum_{n=1}^B \\text{loss}(x^{(n)}, y^{(n)}) $  \n",
    "> Until $ Err $ has converged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732a9b32",
   "metadata": {},
   "source": [
    "## Minimizing the Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68750f98",
   "metadata": {},
   "source": [
    "- Mini-batch updates minimize the **batch error**:\n",
    "    $$\n",
    "    \\text{BatchErr}\\left(f(X;W), g(X)\\right) = \\frac{1}{B} \\sum_{i=1}^B loss\\left(f(x^{(i)}; W), y^{(i)}\\right)\n",
    "    $$\n",
    "\n",
    "- **Expected batch error = Expected loss:**\n",
    "    $$\n",
    "    E\\left[\\text{BatchErr}\\right] = E\\left[loss\\left(f(X;W), g(X)\\right)\\right]\n",
    "    $$\n",
    "\n",
    "- **Variance of the empirical error** (reduces with larger batch size):\n",
    "    $$\n",
    "    var(E) = \\frac{var(loss)}{B}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd6b83",
   "metadata": {},
   "source": [
    "## Key Advantages & Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf20aa18",
   "metadata": {},
   "source": [
    "**Advantages:**\n",
    "\n",
    "- **Efficiency on Large Datasets**\n",
    "    - Processes data in smaller, manageable batches.\n",
    "    - Vectorization enables parallel computation.\n",
    "\n",
    "- **Parallel Training on GPUs**\n",
    "    - Multiple mini-batches can be processed in parallel.\n",
    "    - Independent local updates are aggregated into a global model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a0645",
   "metadata": {},
   "source": [
    "**Potential Challenge: Noisy Gradients**\n",
    "- **Solution:** Use larger batch sizes when possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52b0ec",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent with Forward/Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630970ab",
   "metadata": {},
   "source": [
    "**Mini-Batch Gradient Descent with Forward/Backward Propagation**\n",
    "\n",
    "For each batch of data we have input $X$ and outout $Y$ separately\n",
    "\n",
    "> For $\\text{epoch} = 1, \\cdots, k$:  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;**Shuffle** the training data.  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;For $t = 1, ..., m$ (where $m$ = number of batches):  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Forward propagation** on batch $ X^t $:  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$J^t = \\frac{1}{m} \\sum_{n \\in \\text{Batch}_t} L \\left( \\hat{Y}_n^t, Y_n^t \\right) + \\lambda R(W)$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Backpropagation** on $ J^t $ to compute gradients $ dW $.  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For each layer $ l = 1, ..., L $:  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$W^l = W^l - \\alpha \\, dW^l$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e5dd3d",
   "metadata": {},
   "source": [
    "> **Forward pass** (Vectorized implementation):  \n",
    "> Initialize input: $ A^{[0]} = X^t $.  \n",
    "> For each layer $ l = 1, ..., L $:  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;$Z^{[l]} = W^{[l]} A^{[l-1]}$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;$A^{[l]} = f^{[l]} Z^{[l]}$  \n",
    "> $\\hat{Y}^t = A^{[L]} \\quad \\text{(Output of the last layer)}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a49149d",
   "metadata": {},
   "source": [
    "- $ J^t $: Batch loss (with regularization term $ \\lambda R(W) $).  \n",
    "- $ L $: Loss function (e.g., cross-entropy, MSE).  \n",
    "- $ \\alpha $: Learning rate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42a59e4",
   "metadata": {},
   "source": [
    "## Choosing the Mini-Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29838b29",
   "metadata": {},
   "source": [
    "- **Small datasets:** Use **full-batch** gradient descent.\n",
    "- **Large datasets:** Typical sizes—64, 128, 256, or 1024.\n",
    "- **GPU memory constraints:** Ensure batch data + computations fit in memory.\n",
    "- **General rule:** Depends on the optimization landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f67c71a",
   "metadata": {},
   "source": [
    "# Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec30a4",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b46272",
   "metadata": {},
   "source": [
    "The **learning rate ($\\eta$)** is a critical hyperparameter that controls the step size during optimization. It directly influences:\n",
    "- **Convergence speed** – How quickly the model learns.\n",
    "- **Final performance** – Whether the model reaches a good (or optimal) solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa49a2",
   "metadata": {},
   "source": [
    "| Aspect                  | High Learning Rate (η)                           | Low Learning Rate (η)           |\n",
    "|-------------------------|--------------------------------------------------|---------------------------------|\n",
    "| **Convergence Speed**   | Faster initial progress                          | Slower but stable updates       |\n",
    "| **Minima Behavior**     | Risk of overshooting minima                      | Risk of getting stuck           |\n",
    "| **Stability**           | May cause divergence (∞/NaN)                     | Stable convergence              |\n",
    "| **Final Performance**   | Potentially better escape from poor local minima | May converge suboptimally       |\n",
    "| **Typical Use Cases**   | Early training phases                            | Fine-tuning phases              |\n",
    "| **Loss Landscape**      | Better for rugged landscapes                     | Better for smooth landscapes    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e3951",
   "metadata": {},
   "source": [
    "**Practical Guidelines**\n",
    "- **Loss stagnates?** $\\eta$ is **too low**.\n",
    "- **Loss explodes/NaN?** $\\eta$ is **too high**.\n",
    "- **Start small**, then tune until loss decreases smoothly.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/learning_rate_effect.png\" alt=\"learning rate effect\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53945d12",
   "metadata": {},
   "source": [
    "## Learning Rate Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae53eda",
   "metadata": {},
   "source": [
    "### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589ae5ff",
   "metadata": {},
   "source": [
    "**Learning Rate Decay**\n",
    "\n",
    "As training progresses, **reducing $\\eta$** helps refine convergence:\n",
    "- **Early training:** Larger steps for rapid progress.\n",
    "- **Later stages:** Smaller steps for precise optimization.\n",
    "\n",
    "**Benefits:**\n",
    "- **Faster convergence**\n",
    "- **Improved final accuracy**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68c589",
   "metadata": {},
   "source": [
    "**Learning Rate Strategies**\n",
    "\n",
    "- **Adaptive Learning Rate Methods**  \n",
    "    Algorithms like Adagrad, RMSprop, and Adam adjust the learning rate dynamically based on parameters or gradients.\n",
    "\n",
    "    - **Adagrad:** Adapts the learning rate to each parameter, performing smaller updates for frequently occurring features.\n",
    "    - **RMSprop:** Modifies Adagrad by using a moving average of squared gradients to scale the learning rate.\n",
    "    - **Adam:** Combines elements of RMSprop and momentum, adjusting the learning rate based on an exponentially decaying average of past gradients.\n",
    "\n",
    "- **Learning Rate Schedules**  \n",
    "    These methods adjust the learning rate globally (same for all parameters) based on a predefined rule or formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4322cdd",
   "metadata": {},
   "source": [
    "### **Learning Rate Schedules**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32252dd4",
   "metadata": {},
   "source": [
    "#### **Step-wise Decay**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b574d789",
   "metadata": {},
   "source": [
    "The learning rate is reduced by a constant factor after a fixed number of epochs.\n",
    "$$\n",
    "\\text{lr} = \\text{lr}_0 d^{\\lfloor \\frac{1 + \\text{epoch}}{s} \\rfloor}\n",
    "$$\n",
    "Where:\n",
    "- $lr_0$​: Initial learning rate\n",
    "- $d$: Decay rate\n",
    "- $s$: Step size\n",
    "- epoch: Index of the epoch\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/step_decay.png\" alt=\"step-wise learning rate\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c86e7",
   "metadata": {},
   "source": [
    "**Implementation:**\n",
    "\n",
    "```python\n",
    "#tensorflow\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=10000, decay_rate=0.5, staircase=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "#pytorch\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be821732",
   "metadata": {},
   "source": [
    "#### **Reduce on Loss Plateau Decay**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80294e05",
   "metadata": {},
   "source": [
    "This strategy reduces the learning rate when a performance metric (e.g., validation loss) has stopped improving for a set number of epochs.\n",
    "\n",
    "**Advantages:**\n",
    "- Automatically adapts to training dynamics\n",
    "- Prevents premature convergence\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/plateau_decay.png\" alt=\"plateau decay example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b91587",
   "metadata": {},
   "source": [
    "#### **Fixed Learning Rate:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b2bc9",
   "metadata": {},
   "source": [
    "**Simplest** approach where the **rate remains constant** throughout training.\n",
    "\n",
    "**Implementation**\n",
    "```python\n",
    "#tensorflow\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "#pytorch\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e8f95",
   "metadata": {},
   "source": [
    "#### **Time-Based Decay**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb32131",
   "metadata": {},
   "source": [
    "The learning rate decreases over time using a predefined formula, often proportionally to the inverse of the training epoch number.  \n",
    "This helps in taking larger steps at the beginning and finer steps as the model approaches convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0f206",
   "metadata": {},
   "source": [
    "##### **Cosine Decay**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100927e",
   "metadata": {},
   "source": [
    "Smooth reduction following a cosine curve.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\alpha_t = \\frac{1}{2} \\alpha_0 (1 + \\cos(\\frac{2 \\pi}{T}))\n",
    "$$\n",
    "Where:\n",
    "- $\\alpha_0$: Initial learning rate\n",
    "- $\\alpha_t$: Learning rate at epoch t\n",
    "- $T$: Total number of epochs\n",
    "    \n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/lr_decay_cosine.png\" alt=\"cosine learning rate decay\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7471a42",
   "metadata": {},
   "source": [
    "##### **Linear Decay**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f57e6fb",
   "metadata": {},
   "source": [
    "Straightforward linear reduction.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\alpha_t = \\alpha_0(1 - \\frac{t}{T})\n",
    "$$\n",
    "Where:\n",
    "- $\\alpha_0$: Initial learning rate\n",
    "- $\\alpha_t$: Learning rate at epoch t\n",
    "- $T$: Total number of epochs\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/lr_decay_linear.png\" alt=\"linear learning rate decay\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f99eb18",
   "metadata": {},
   "source": [
    "##### **Inverse Square Root Decay**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ccda0",
   "metadata": {},
   "source": [
    "Aggressive early decay with slower reduction later.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\alpha_t = \\frac{\\alpha_0}{\\sqrt{t}}\n",
    "$$\n",
    "Where:\n",
    "- $\\alpha_0$: Initial learning rate\n",
    "- $\\alpha_t$: Learning rate at epoch t\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/lr_decay_inverse_sqrt.png\" alt=\"inverse sqrt learning rate decay\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6169e40",
   "metadata": {},
   "source": [
    "##### **Exponential Decay**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff906d",
   "metadata": {},
   "source": [
    "Continuous exponential reduction.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\alpha_t = \\alpha_0 e^{t}\n",
    "$$\n",
    "Where:\n",
    "- $\\alpha_0$: Initial learning rate\n",
    "- $\\alpha_t$: Learning rate at epoch t\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/lr_decay_exponential.png\" alt=\"exponential learning rate decay\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ab99e",
   "metadata": {},
   "source": [
    "**Implementation:**\n",
    "```python\n",
    "#tensorflow\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=10000, decay_rate=0.9, staircase=False)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "#pytorch\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee8f31c",
   "metadata": {},
   "source": [
    "##### **Polynomial**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4a2c71",
   "metadata": {},
   "source": [
    "Flexible decay controlled by power parameter.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\alpha_t = \\alpha_0(1 - \\frac{t}{T})^m\n",
    "$$\n",
    "Where:\n",
    "- $\\alpha_0$: Initial learning rate\n",
    "- $\\alpha_t$: Learning rate at epoch t\n",
    "- $T$: Total number of epochs\n",
    "- $m$: Power hyperparameter\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/all_decay_schedules.png\" alt=\"all schedules\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac23512f",
   "metadata": {},
   "source": [
    "#### **Cyclical Learning Rate (CLR)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ba837",
   "metadata": {},
   "source": [
    "**Cyclical Learning Rate**\n",
    "\n",
    "This method allows the learning rate to oscillate between a minimum and a maximum boundary.  \n",
    "It can be implemented using various functions such as triangular, sinusoidal, or exponential patterns.\n",
    "\n",
    "**Benefits:**\n",
    "- Escapes local minima\n",
    "- Automates learning rate tuning\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/clr.png\" alt=\"clr learning rate decay\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49ec7f4",
   "metadata": {},
   "source": [
    "**Implementation:**\n",
    "```python\n",
    "#pytorch\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff41d1b",
   "metadata": {},
   "source": [
    "#### **Learning Rate Warmup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157b27f",
   "metadata": {},
   "source": [
    "Starts with a small learning rate and gradually *(e.g., linearly)* increases it over a few initial epochs or iterations.  \n",
    "This is particularly useful in preventing the model from diverging in the initial phase of training.  \n",
    "Often used in training deep learning\n",
    "\n",
    "**Use Cases:**\n",
    "- Transformer models\n",
    "- Large batch training\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/linear_warmup.png\" alt=\"linear warmup learning rate decay\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
