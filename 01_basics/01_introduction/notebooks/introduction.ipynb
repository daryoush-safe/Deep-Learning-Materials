{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6014fad3",
   "metadata": {},
   "source": [
    "# Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c3531",
   "metadata": {},
   "source": [
    "**What is Deep Learning?**\n",
    "\n",
    "Deep learning is a subfield of machine learning that uses **deep models**\n",
    "\n",
    "**Deep models:** computational models consisting of multiple processing layers to learn hierarchical representations of data.  \n",
    "\n",
    "Each layer transforms the input into a more abstract and meaningful representation, where the output of one layer serves as the input to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d80e4c",
   "metadata": {},
   "source": [
    "# Comparison with Traditional Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fb1b81",
   "metadata": {},
   "source": [
    "**Machine Learning Recap:**\n",
    "- Learns a mapping from input features to output labels using training data.\n",
    "- Requires **hand-crafted** features to perform well.\n",
    "- Performance depends heavily on the **quality of the data** representation.\n",
    "\n",
    "However, designing good features, especially for complex inputs (e.g., images, video, and time series) is challenging:\n",
    "- It is difficult to know which features should be extracted\n",
    "- Experts may spend years refining feature sets that are still incomplete or over-specified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a80885",
   "metadata": {},
   "source": [
    "**Deep Learning Approach:**\n",
    "\n",
    "Deep learning automatically learns both:\n",
    "- Feature representations from raw input data\n",
    "- The mapping from these features to the output\n",
    "\n",
    "$$\\text{Input} \\rightarrow \\underbrace{\\text{Trainable Feature Extractor} \\rightarrow \\text{Trainable Classifier}}_{\\text{End-to-End Learning}} \\rightarrow \\text{Output}$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/deep_example.png\" alt=\"deep model example\">\n",
    "</div>\n",
    "\n",
    "In this schematic:\n",
    "- First layer ($h_i$): learns features\n",
    "- Second layer ($y_i$): performs classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e62626",
   "metadata": {},
   "source": [
    "# Multi-Layer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b06f56",
   "metadata": {},
   "source": [
    "For a layer $ l $, result after the activation $f$ of the $ k $-th neuron is computed as:\n",
    "$$a_k^{[l]} = f \\left( \\sum_{i=1}^M W_{ki}^{[l]} a_i^{[l-1]} + {b_k}^{[l]} \\right)$$\n",
    "\n",
    "Where:\n",
    "- $ k \\in \\{1, \\dots, K\\} $: Index of the $ k $-th neuron in layer $ l $.\n",
    "\n",
    "- $ W_{ki}^{[l]} $: Weight connecting the $ i $-th neuron in layer $ l-1 $ to the $ k $-th neuron in layer $ l $.\n",
    "\n",
    "- $ b_k^{[l]} $: Bias term for the $ k $-th neuron in layer $ l $.\n",
    "\n",
    "- $ f(\\cdot) $: Activation function (e.g., ReLU, sigmoid).\n",
    "\n",
    "- $ i \\in \\{0, \\dots, M\\} $: Index of neurons in the previous layer.\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/mlp_represent.png\" alt=\"multi-layer neural network example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c430773",
   "metadata": {},
   "source": [
    "**Learnable parameters:**\n",
    "\n",
    "The learnable parameters in an MLP are:\n",
    "- Weights $ W_{ki}^{[l]} $\n",
    "- Biases $ b_k^{[l]} $\n",
    "\n",
    "The total number of parameters in a network with $ L $ layers is:\n",
    "\n",
    "$$(d+1)m_1 + (m_1+1)m_2 + \\dots + (m_L+1)k$$\n",
    "\n",
    "Where:\n",
    "- $ d $: Number of input features.\n",
    "- $ m_i $: Number of neurons in the $ i $-th layer.\n",
    "- $ k $: Number of output units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f9b091",
   "metadata": {},
   "source": [
    "**Compositionality in Deep Learning**\n",
    "\n",
    "Deep learning breaks down complex mappings into simpler, nested functions:\n",
    "$$\\text{Input} \\rightarrow \\underbrace{\\text{layer 1} \\rightarrow \\cdots \\rightarrow \\text{layer n}}_{\\text{Trainable Feature Extractor}} \\rightarrow \\text{Classifier} \\rightarrow \\text{Output}$$\n",
    "\n",
    "Why is this powerful?\n",
    "- Each layer captures increasingly abstract features\n",
    "- **Early layers:** **low-level** features (e.g., edges, corners)\n",
    "- **Later layers:** **high-level** concepts (e.g., faces, objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c459c83",
   "metadata": {},
   "source": [
    "Compositionality is useful to describe the world around us efficiently\n",
    "- Learned function seen as a composition of simpler operations\n",
    "- Hierarchy of features, concepts, leading to more abstract factors enabling better generalization\n",
    "    - each concept defined in relation to simpler concepts\n",
    "    - more abstract representations computed in terms of less abstract ones.\n",
    "- Again, theory shows this can be **exponentially advantageous**\n",
    "\n",
    "Deep learning has great power and flexibility by learning to represent the world as a nested hierarchy of concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0badb564",
   "metadata": {},
   "source": [
    "# History of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b47dd4",
   "metadata": {},
   "source": [
    "- **1943 Artificial Neuron**\n",
    "    - Modeled brain neurons using a weighted sum of **boolean** inputs passed through an activation function.\n",
    "\n",
    "- **1957 Perceptron**\n",
    "    - A linear classifier for real inputs, limited to **linearly separable** problems (e.g., unable to solve XOR).\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/perceptron_schematic.png\" alt=\"Perceptron example\">\n",
    "</div>\n",
    "\n",
    "- **1969 Limitations of Neural Networks:**\n",
    "    - The perceptron’s algorithm can't handle non-linearly separable problems,\n",
    "\n",
    "- **1979 Neocognitron (inspires CNNs):**\n",
    "    - Inspired by the brain’s visual system, it introduced multiple computational layers, laying the groundwork for convolutional neural networks (CNNs).\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/cnn.png\" alt=\"CNN example\">\n",
    "</div>\n",
    "\n",
    "- **1982 Recurrent Neural Networks (RNNs):**\n",
    "    - Designed for sequential and time-series data\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/rnn.png\" alt=\"RNN example\">\n",
    "</div>\n",
    "\n",
    "- **1986 Back Propagation:**\n",
    "    - Combined gradient descent, the chain rule, and dynamic programming to train neural networks efficiently (building on automatic differentiation from 1970).\n",
    "\n",
    "- **1997 Long Short-Term Memory (LSTM)**\n",
    "    - Enhanced RNNs to better handle long-term dependencies in sequential data.\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/lstm.png\" alt=\"LSTM example\">\n",
    "</div>\n",
    "\n",
    "- **1998 LeNet (Neocognitron + Back-prop)**\n",
    "    - Combined Neocognitron’s architecture with back-propagation to solve handwritten digit recognition, marking a practical deep learning success.\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../assets/le_net.png\" alt=\"LeNet example\">\n",
    "</div>\n",
    "\n",
    "- **2006 Deep Learning**\n",
    "    - The training of each layer individually is an easier undertaking\n",
    "        - Training multi layered neural networks became easier\n",
    "        - Per-layer trained parameters initialize further training\n",
    "    - Resource and data limited\n",
    "    - Layer-wise pre-training simplified training of deep networks, revitalizing interest in neural networks.\n",
    "\n",
    "- **2009 ImageNet**\n",
    "    - Introduced a large-scale image dataset, enabling deep learning to tackle complex vision tasks\n",
    "\n",
    "- **2012 AlexNet**\n",
    "    - A deep CNN that achieved breakthrough performance on ImageNet, leveraging GPUs and large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86276b1",
   "metadata": {},
   "source": [
    "**Why Does Deep Learning Become Popular?**\n",
    "\n",
    "- **Data:**\n",
    "    - Availability of massive datasets\n",
    "    - provided the volume of training examples needed for deep models\n",
    "    - E.g., ImageNet\n",
    "\n",
    "- **Hardware:**\n",
    "    - Availability of the computational resources to run much larger models\n",
    "    - Specially GPU\n",
    "\n",
    "- **Algorithm:**\n",
    "    - New architectures (CNNs, RNNs)\n",
    "    - Frameworks like (Tensorflow or Pytorch)\n",
    "    - New training techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5217355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do not forget GRU in the RNN !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5bf438",
   "metadata": {},
   "source": [
    "# Advanced Concepts in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f31858",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c3ddc",
   "metadata": {},
   "source": [
    "**Transfer Learning**\n",
    "\n",
    "- **Problem:**\n",
    "    - **Training** deep networks from scratch **requires massive labeled data**.\n",
    "\n",
    "- **Solution:**\n",
    "    - Use **pre-trained** models (e.g., ResNet, BERT) as feature extractors.\n",
    "\n",
    "- **Example:**\n",
    "    - After image classification, achievements were obtained in other vision tasks:\n",
    "        - Object detection\n",
    "        - Segmentation\n",
    "        - Image captioning\n",
    "        - Visual Question Answering (VQA)\n",
    "        - …"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e8d321",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574e4d6",
   "metadata": {},
   "source": [
    "Introduced for sequence-to-sequence tasks in natural language processing (NLP), transformers consist of:\n",
    "\n",
    "- **Encoder:** Encodes input sequences into a latent representation.\n",
    "- **Decoder:** Generates output sequences from the encoded representation.\n",
    "\n",
    "**Impact:**  \n",
    "Dominates NLP (GPT, BERT) and vision (ViT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db05538",
   "metadata": {},
   "source": [
    "## Self-Supervised Learning (SSL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edcec13",
   "metadata": {},
   "source": [
    "**Idea:** Generate **pretext tasks** from unlabeled data to learn useful representations.\n",
    "\n",
    "**Benefits:**\n",
    "- Leverages vast unlabeled data.\n",
    "- Produces more generalizable models.\n",
    "\n",
    "The learning mechanism is the same as supervised learning, but instead of tagging the data manually, the model itself estimates the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc33d53b",
   "metadata": {},
   "source": [
    "## Multi-Modal Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f7044",
   "metadata": {},
   "source": [
    "Multi-modal models learn from multiple data types (e.g., image + text)\n",
    "\n",
    "**CLIP:** Learns a multi-modal embedding space by jointly training an image encoder and text encoder (map image and text)\n",
    "\n",
    "- Using the available large amount of multi-modal data\n",
    "- Zero-shot classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da049ac0",
   "metadata": {},
   "source": [
    "## Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552caf09",
   "metadata": {},
   "source": [
    "Output text, image, video, audio, …. given no condition or a partial guidance or prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd5017",
   "metadata": {},
   "source": [
    "## Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c98d7be",
   "metadata": {},
   "source": [
    "Large language models are one of the most successful applications of transformer models.\n",
    "\n",
    "LLMs, built on **transformer** architectures, are trained on massive text datasets in a **self-supervised** manner.\n",
    "- Recognize\n",
    "- Summarize\n",
    "- Translate\n",
    "- Predict\n",
    "- Generate text and other content based on knowledge gained from massive datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
